# MCP Spider Server

An MCP (Model Context Protocol) spider server based on [crawl4ai](https://github.com/unclecode/crawl4ai) that provides powerful web crawling and content extraction capabilities.

## ğŸš€ Features

- **æ™ºèƒ½ç½‘ç»œçˆ¬è™«**: åŸºäº crawl4ai çš„é«˜æ•ˆç½‘é¡µå†…å®¹æå–
- **LLM å¢å¼ºæå–**: é›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒæ™ºèƒ½å†…å®¹ç†è§£å’Œç»“æ„åŒ–æå–
- **å¤šæ ¼å¼è¾“å‡º**: æ”¯æŒ HTMLã€Markdownã€JSONã€PDF å’Œ PNG æ ¼å¼
- **æ™ºèƒ½å†…å®¹æå–**: ä½¿ç”¨ LLMExtractionStrategy è¿›è¡ŒåŸºäºè¯­ä¹‰çš„å†…å®¹æå–
- **çµæ´»é…ç½®**: æ”¯æŒä¼ ç»Ÿ CSS é€‰æ‹©å™¨æå–å’Œ LLM æ™ºèƒ½æå–ä¸¤ç§æ¨¡å¼
- **æˆªå›¾åŠŸèƒ½**: è‡ªåŠ¨ç”Ÿæˆç½‘é¡µæˆªå›¾
- **PDF å¯¼å‡º**: å°†ç½‘é¡µå†…å®¹å¯¼å‡ºä¸º PDF æ–‡ä»¶
- **å†…å®¹è¿‡æ»¤**: ä½¿ç”¨ PruningContentFilter ä¼˜åŒ–å†…å®¹æå–
- **ç»“æ„åŒ–æ•°æ®æå–**: æ”¯æŒ JsonCssExtractionStrategy ç²¾ç¡®æ•°æ®æå–
- **æ–‡ä»¶ä¸‹è½½**: è‡ªåŠ¨ä¸‹è½½å¹¶ä¿å­˜å¼•ç”¨æ–‡ä»¶
- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Ollamaã€OpenAIã€Claude ç­‰å¤šç§ LLM æä¾›å•†
- **ç¯å¢ƒå˜é‡é…ç½®**: çµæ´»çš„æ¨¡å‹é…ç½®å’Œåˆ‡æ¢æœºåˆ¶
- **MCP åè®®æ”¯æŒ**: å®Œå…¨å…¼å®¹ MCP æ ‡å‡†ï¼Œå¯é›†æˆåˆ°æ”¯æŒ MCP çš„å®¢æˆ·ç«¯

## ğŸ“¦ Installation

### Requirements

- Python 3.8+
- Virtual environment recommended
- **LLM ä¾èµ–**: 
  - `litellm` - ç»Ÿä¸€çš„å¤šæ¨¡å‹ LLM æ¥å£
  - Ollama æˆ–å…¶ä»– LLM æä¾›å•† (å¯é€‰ï¼Œç”¨äºæ™ºèƒ½å†…å®¹æå–)
- **æµè§ˆå™¨ä¾èµ–**: 
  - Chrome/Chromium (crawl4ai éœ€è¦)

### Installation Steps

1. **Clone repository**
```bash
git clone https://github.com/osins/crawler-mcp-server.git
cd crawler-mcp-server
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows
```

3. **Install dependencies**
```bash
pip install -e .
```

## ğŸ”§ MCP Service Configuration

### Claude Desktop Configuration Example

Add the following configuration to Claude Desktop's configuration file:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "spider": {
      "command": "/path/to/crawler-mcp-server/venv/bin/python",
      "args": [
        "/path/to/crawler-mcp-server/spider_mcp_server/server.py"
      ],
      "description": "MCP spider server using crawl4ai for web crawling and content extraction"
    }
  }
}
```

### General MCP Client Configuration

If you use other MCP clients, you can use the following general configuration:

```json
{
  "servers": {
    "spider-crawler": {
      "name": "Spider Crawler Server",
      "description": "Web crawling and content extraction server",
      "command": "/path/to/crawler-mcp-server/venv/bin/python",
      "args": [
        "/path/to/crawler-mcp-server/spider_mcp_server/server.py"
      ],
      "timeout": 30000
    }
  }
}
```

### ğŸ“‹ Configuration Notes

**Direct script execution is all you need:**
- No environment variables needed
- No need to use `-m` parameter
- Python automatically handles relative imports
- Most simple and reliable configuration

### ğŸ¤– LLM é…ç½®é€‰é¡¹

ä¸ºäº†å¯ç”¨ LLM å¢å¼ºåŠŸèƒ½ï¼Œå¯ä»¥è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# å¯ç”¨ LLM æ¨¡å¼
export CRAWL_MODE=llm

# LLM æä¾›å•†é…ç½®
export LLAMA_PROVIDER="ollama/qwen2.5-coder:latest"  # é»˜è®¤å€¼
export LLAMA_API_TOKEN="your_api_token"             # å¯é€‰ï¼ŒæŸäº›æä¾›å•†éœ€è¦
export LLAMA_BASE_URL="http://localhost:11434"       # å¯é€‰ï¼Œè‡ªå®šä¹‰ API ç«¯ç‚¹
export LLAMA_MAX_TOKENS=4096                         # å¯é€‰ï¼Œæœ€å¤§ token æ•°
```

**æ”¯æŒçš„ LLM æä¾›å•†:**
- **Ollama**: `ollama/model-name` (æœ¬åœ°éƒ¨ç½²)
- **OpenAI**: `openai/gpt-4` / `openai/gpt-3.5-turbo`
- **Claude**: `anthropic/claude-3-sonnet`
- **å…¶ä»–**: é€šè¿‡ litellm æ”¯æŒçš„æ‰€æœ‰æä¾›å•†

## ğŸ› ï¸ MCP Protocol Usage Guide

### Client Development Based on MCP Protocol

This project is based on the [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) protocol and provides standardized tool call interfaces. Here are the key points for writing MCP clients:

#### 1. MCP Connection Establishment

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Configure server parameters
server_params = StdioServerParameters(
    command="/path/to/venv/bin/python",  # Python interpreter path
    args=["/path/to/server.py"]         # Server script path
)

# Establish stdio connection
async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        await session.initialize()  # Initialize session
```

#### 2. Tool Calls and Result Handling

**âš ï¸ Important: MCP Return Value Structure**

The MCP server returns a `CallToolResult` object, with the actual content in `result.content`:

```python
# âŒ Incorrect approach (common error)
for content in result:  # result is not iterable
    print(content.text)

# âœ… Correct approach
result = await session.call_tool("tool_name", {"param": "value"})
for content in result.content:  # Access content attribute
    if content.type == "text":  # Check content type
        print(content.text)
```

#### 3. Tool Interfaces for This Project

**Available Tools:**
- `say_hello` - Test connection
- `echo_message` - Echo message  
- `crawl_web_page` - Web page crawling

**crawl_web_page Tool Parameters:**
```python
{
    "url": "https://example.com",           # URL to crawl
    "save_path": "./output_directory"       # Save path
}
```

**Return Value Handling:**
```python
result = await session.call_tool("crawl_web_page", {
    "url": "https://github.com/unclecode/crawl4ai",
    "save_path": "./results"
})

# Parse return results correctly
for content in result.content:
    if content.type == "text":
        message = content.text
        print(f"Crawling result: {message}")
        
        # Message format example:
        # "Successfully crawled https://github.com/unclecode/crawl4ai and saved 8 files to ./results/20231119-143022"
```

#### 4. Error Handling Best Practices

```python
async def safe_crawl(session: ClientSession, url: str, save_path: str):
    try:
        result = await session.call_tool("crawl_web_page", {
            "url": url,
            "save_path": save_path
        })
        
        # Check return results
        if result.content:
            for content in result.content:
                if content.type == "text":
                    if "Failed to crawl" in content.text:
                        print(f"âŒ Crawling failed: {content.text}")
                    else:
                        print(f"âœ… Crawling successful: {content.text}")
        else:
            print("âŒ No return result received")
            
    except Exception as e:
        print(f"âŒ Tool call failed: {e}")
```

## ğŸ› ï¸ Available Tools

### 1. `crawl_web_page`

Crawl webpage content from the specified URL and save in multiple formats. Supports both traditional CSS extraction and LLM-enhanced extraction modes.

**Parameters:**
- `url` (string, required): The webpage URL to crawl
- `save_path` (string, required): The directory path to save crawled content

**Features:**
- Automatic webpage screenshot (PNG)
- PDF export generation
- Raw Markdown content extraction
- Cleaned/filtered Markdown content
- Structured data extraction (JSON)
- HTML content preservation
- Downloaded files processing
- **LLM æ™ºèƒ½æå–** (å½“è®¾ç½® CRAWL_MODE=llm æ—¶å¯ç”¨):
  - åŸºäºè¯­ä¹‰çš„å†…å®¹ç†è§£
  - è‡ªåŠ¨å»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰éä¸»è¦å†…å®¹
  - ç»“æ„åŒ–çš„ Markdown è¾“å‡º
  - æ”¯æŒå¤šç§ LLM æä¾›å•†

**Example usage:**
```python
# ä¼ ç»Ÿçˆ¬å–æ¨¡å¼
result = await session.call_tool("crawl_web_page", {
    "url": "https://example.com",
    "save_path": "./output_directory"
})

# LLM å¢å¼ºæ¨¡å¼ (è®¾ç½®ç¯å¢ƒå˜é‡)
os.environ["CRAWL_MODE"] = "llm"
os.environ["LLAMA_PROVIDER"] = "ollama/qwen2.5-coder:latest"
os.environ["LLAMA_BASE_URL"] = "http://localhost:11434"
```

### 2. `say_hello`

Simple greeting tool for testing server connectivity.

**Parameters:**
- `name` (string, optional): Name to greet, defaults to "World"

**Example usage:**
```python
result = await session.call_tool("say_hello", {
    "name": "Alice"
})
```

### 3. `echo_message`

Echo messages back to test communication.

**Parameters:**
- `message` (string, required): The message to echo

**Example usage:**
```python
result = await session.call_tool("echo_message", {
    "message": "Hello MCP!"
})
```

## ğŸ“ Output File Structure

After crawling, the following files will be generated in the specified output directory:

```
output_directory/
â”œâ”€â”€ output.html              # Complete HTML content
â”œâ”€â”€ output.json              # Structured data (CSS-extracted JSON)
â”œâ”€â”€ output.png               # Webpage screenshot
â”œâ”€â”€ output.pdf               # PDF version of the page
â”œâ”€â”€ raw_markdown.md          # Raw markdown extraction
â”œâ”€â”€ fit_markdown.md          # Cleaned/filtered markdown
â”œâ”€â”€ downloaded_files.json    # List of downloaded files (if any)
â””â”€â”€ files/                   # Downloaded files directory (if any)
```

## ğŸ” Usage Examples

### Basic Web Crawling

```python
import asyncio
import os
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def crawl_example():
    # Configure server connection parameters (adjust paths as needed)
    project_root = Path("/path/to/your/crawler-mcp-server")
    server_params = StdioServerParameters(
        command=str(project_root / "venv" / "bin" / "python"),
        args=[str(project_root / "spider_mcp_server" / "server.py")]
    )
    
    # Create output directory
    output_dir = "./crawl_results"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # Connect to MCP server
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize session
                await session.initialize()
                
                # Call crawling tool
                result = await session.call_tool("crawl_web_page", {
                    "url": "https://github.com/unclecode/crawl4ai",
                    "save_path": output_dir
                })
                
                # âœ… Correct return value handling
                # MCP server returns CallToolResult object, content is in result.content
                for content in result.content:
                    if content.type == "text":
                        print(f"âœ… Crawling result: {content.text}")
                        
    except Exception as e:
        print(f"âŒ Crawling failed: {e}")

# Run example
asyncio.run(crawl_example())
```

### Batch Crawling Multiple Webpages

```python
urls = [
    "https://example.com",
    "https://github.com", 
    "https://stackoverflow.com"
]

for i, url in enumerate(urls):
    result = await session.call_tool("crawl_web_page", {
        "url": url,
        "save_path": f"./results/crawl_{i+1}"
    })
    
    # âœ… Correct return value handling
    for content in result.content:
        if content.type == "text":
            print(f"Crawled: {url} - {content.text}")
    
    # Add delay to avoid too frequent requests
    await asyncio.sleep(2)
```

### LLM å¢å¼ºçˆ¬å–ç¤ºä¾‹

```python
import os
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def llm_enhanced_crawl():
    # é…ç½® LLM ç¯å¢ƒå˜é‡
    os.environ["CRAWL_MODE"] = "llm"
    os.environ["LLAMA_PROVIDER"] = "ollama/qwen2.5-coder:latest"
    os.environ["LLAMA_BASE_URL"] = "http://localhost:11434"
    
    # é…ç½®æœåŠ¡å™¨è¿æ¥
    server_params = StdioServerParameters(
        command="/path/to/venv/bin/python",
        args=["/path/to/spider_mcp_server/server.py"]
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            
            # LLM å¢å¼ºçˆ¬å–
            result = await session.call_tool("crawl_web_page", {
                "url": "https://example.com/article",
                "save_path": "./llm_results"
            })
            
            for content in result.content:
                if content.type == "text":
                    print(f"LLM å¢å¼ºçˆ¬å–ç»“æœ: {content.text}")

asyncio.run(llm_enhanced_crawl())
```

## ğŸ§ª Development and Testing

### Run Tests

The project includes a comprehensive test suite:

```bash
# Run complete crawler test with real file output
python test/test_complete_crawler.py

# Run individual component tests
python test/test_hello.py      # Test hello/echo functions
python test/test_server.py     # Test MCP server functionality
python test/test_crawl.py      # Test crawling functions
python test/test_complete.py    # Test complete workflow
```

### Test Directory Structure

```
test/
â”œâ”€â”€ test_complete_crawler.py    # Full integration test
â”œâ”€â”€ test_hello.py             # Hello/echo functionality
â”œâ”€â”€ test_server.py            # MCP server protocol
â”œâ”€â”€ test_crawl.py             # Core crawling logic
â””â”€â”€ test_complete.py          # End-to-end workflow
```

### Development Mode

```bash
# Install development dependencies
pip install -e ".[dev]"

# The project uses pyright for type checking (configured in pyproject.toml)
# No additional linting tools are configured
```

## ğŸ“š Project Structure

```
crawler-mcp-server/
â”œâ”€â”€ spider_mcp_server/          # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ server.py              # MCP server implementation
â”‚   â”œâ”€â”€ crawl.py              # Crawling logic and file handling
â”‚   â”œâ”€â”€ llm.py                # LLM configuration and extraction strategies
â”‚   â””â”€â”€ utils.py              # Utility functions for file I/O
â”œâ”€â”€ test/                     # Test suite
â”‚   â”œâ”€â”€ test_litellm_ollama.py # LLM integration tests
â”‚   â””â”€â”€ ...                   # Other test files
â”œâ”€â”€ test_output/              # Test output directory
â”œâ”€â”€ typings/                  # Type stubs for crawl4ai
â”œâ”€â”€ pyproject.toml            # Project configuration
â””â”€â”€ README.md                # This file
```

## ğŸ“š API Reference

### Core Classes and Functions

#### `llm_config()` function (`llm.py`)
é…ç½® LLM å¢å¼ºçˆ¬å–ç­–ç•¥ã€‚

**å‚æ•°:**
- `instruction` (str): æå–æŒ‡ä»¤ï¼Œé»˜è®¤ä¸ºä¸“é—¨ä¼˜åŒ–çš„ç½‘é¡µå†…å®¹æå–æŒ‡ä»¤

**è¿”å›:**
- `CrawlerRunConfig`: é…ç½®äº† LLM æå–ç­–ç•¥çš„çˆ¬å–é…ç½®

**ç‰¹æ€§:**
- æ”¯æŒ litellm çš„æ‰€æœ‰æä¾›å•†
- è‡ªåŠ¨åˆ†å—å¤„ç†å¤§å‹å†…å®¹
- æ™ºèƒ½å†…å®¹è¿‡æ»¤å’Œç»“æ„åŒ–è¾“å‡º
- å¯é…ç½®çš„æ¸©åº¦å’Œ token å‚æ•°

#### `save()` function (`utils.py`)
Save content to files with proper encoding handling.

**Parameters:**
- `path`: Directory path
- `name`: Filename
- `s`: Content (string, bytes, or bytearray)
- `call`: Callback function with saved file path

#### `saveJson()` function (`crawl.py`)
Async function to save downloaded files information and handle file downloads.

**Features:**
- Saves `downloaded_files.json` with file metadata
- Downloads and saves referenced files to `files/` subdirectory
- Error handling for failed downloads

#### `crawl_config()` function (`crawl.py`)
åŠ¨æ€é€‰æ‹©çˆ¬å–é…ç½®ï¼Œæ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å¯ç”¨ LLM æ¨¡å¼ã€‚

**ç¯å¢ƒå˜é‡:**
- `CRAWL_MODE=llm`: å¯ç”¨ LLM å¢å¼ºæå–
- å…¶ä»–å€¼: ä½¿ç”¨ä¼ ç»Ÿ CSS é€‰æ‹©å™¨æå–

## ğŸ¯ Configuration Details

### LLM æå–ç­–ç•¥

å½“å¯ç”¨ LLM æ¨¡å¼æ—¶ï¼Œä½¿ç”¨ä»¥ä¸‹æ™ºèƒ½æå–é…ç½®ï¼š

**é»˜è®¤æå–æŒ‡ä»¤:**
```
You are a **Web Content Extraction Assistant**. Your task is to extract the **complete, clean, and precise main text content** from a given web page...
```

**LLM é…ç½®å‚æ•°:**
- `provider`: é€šè¿‡ `LLAMA_PROVIDER` ç¯å¢ƒå˜é‡é…ç½®
- `api_token`: é€šè¿‡ `LLAMA_API_TOKEN` ç¯å¢ƒå˜é‡é…ç½®
- `base_url`: é€šè¿‡ `LLAMA_BASE_URL` ç¯å¢ƒå˜é‡é…ç½®  
- `max_tokens`: é»˜è®¤ 4096ï¼Œå¯é€šè¿‡ `LLAMA_MAX_TOKENS` è°ƒæ•´
- `temperature`: 0.1 (ç¡®ä¿è¾“å‡ºç¨³å®šæ€§)
- `chunk_token_threshold`: 1400 (åˆ†å—å¤„ç†é˜ˆå€¼)
- `apply_chunking`: true (å¯ç”¨å†…å®¹åˆ†å—)

### CSS Extraction Strategy

ä¼ ç»Ÿæ¨¡å¼ä½¿ç”¨é¢„é…ç½®çš„ CSS æå–æ¨¡å¼ï¼š

```javascript
{
  "baseSelector": "body",
  "fields": [
    {"name": "title", "selector": "h2", "type": "text"},
    {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
    {"name": "p", "selector": "p", "type": "text"}
  ]
}
```

### Content Filtering

ä½¿ç”¨ `PruningContentFilter` é…ç½®ï¼š
- `threshold`: 0.35 (åŠ¨æ€é˜ˆå€¼)
- `min_word_threshold`: 3
- `threshold_type`: "dynamic"

### Browser Configuration

- Headless mode enabled
- JavaScript enabled  
- Bypass cache for fresh content

## âš ï¸ Important Notes

1. **Rate Limiting**: Please control crawling frequency reasonably to avoid excessive pressure on target websites
2. **robots.txt**: Please comply with robots.txt rules of target websites
3. **Legal Compliance**: Ensure crawling behavior complies with relevant laws, regulations and website terms of use
4. **Browser Requirements**: crawl4ai requires a browser engine (Chrome/Chromium) to be installed on the system
5. **Memory Usage**: Large screenshots and PDFs may consume significant memory and disk space

## ğŸ¤ Contributing

Welcome to submit Issues and Pull Requests!

1. Fork this repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

This project uses MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Related Links

- [crawl4ai Official Documentation](https://github.com/unclecode/crawl4ai)
- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [Claude Desktop Documentation](https://docs.anthropic.com/claude/docs/overview)
- [LiteLLM Documentation](https://docs.litellm.ai/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Project Package](https://pypi.org/project/spider-mcp/)

## ğŸ“ Support

If you encounter problems or have suggestions:

1. **Check Issues**: [GitHub Issues](https://github.com/osins/crawler-mcp-server/issues)
2. **Create New Issue**: Report bugs or request features
3. **Test First**: Run `python test/test_complete_crawler.py` to verify setup

## ğŸ® CLI Entry Point

The package includes a CLI entry point:

```bash
# After installation
spider-mcp
```

This is equivalent to running:
```bash
python -m spider_mcp_server.server
```

## ğŸ› ï¸ Available Tools

---

**Made with â¤ï¸ using crawl4ai and MCP**

*Current Version: 0.1.0*