"""
This type stub file was generated by pyright.
"""

from typing import Any, AsyncGenerator, Callable, Dict, List, Optional, Union
from .async_configs import BrowserConfig, CrawlerRunConfig
from .models import CrawlResult

class Crawl4aiClientError(Exception):
    """Base exception for Crawl4ai Docker client errors."""
    ...


class ConnectionError(Crawl4aiClientError):
    """Raised when connection to the Docker server fails."""
    ...


class RequestError(Crawl4aiClientError):
    """Raised when the server returns an error response."""
    ...


class Crawl4aiDockerClient:
    """Client for interacting with Crawl4AI Docker server with token authentication."""
    def __init__(self, base_url: str = ..., timeout: float = ..., verify_ssl: bool = ..., verbose: bool = ..., log_file: Optional[str] = ...) -> None:
        ...
    
    async def authenticate(self, email: str) -> None:
        """Authenticate with the server and store the token."""
        ...
    
    async def crawl(self, urls: List[str], browser_config: Optional[BrowserConfig] = ..., crawler_config: Optional[CrawlerRunConfig] = ..., hooks: Optional[Union[Dict[str, Callable], Dict[str, str]]] = ..., hooks_timeout: int = ...) -> Union[CrawlResult, List[CrawlResult], AsyncGenerator[CrawlResult, None]]:
        """
        Execute a crawl operation.

        Args:
            urls: List of URLs to crawl
            browser_config: Browser configuration
            crawler_config: Crawler configuration
            hooks: Optional hooks - can be either:
                   - Dict[str, Callable]: Function objects that will be converted to strings
                   - Dict[str, str]: Already stringified hook code
            hooks_timeout: Timeout in seconds for each hook execution (1-120)

        Returns:
            Single CrawlResult, list of results, or async generator for streaming

        Example with function hooks:
            >>> async def my_hook(page, context, **kwargs):
            ...     await page.set_viewport_size({"width": 1920, "height": 1080})
            ...     return page
            >>>
            >>> result = await client.crawl(
            ...     ["https://example.com"],
            ...     hooks={"on_page_context_created": my_hook}
            ... )
        """
        ...
    
    async def get_schema(self) -> Dict[str, Any]:
        """Retrieve configuration schemas."""
        ...
    
    async def close(self) -> None:
        """Close the HTTP client session."""
        ...
    
    async def __aenter__(self) -> Crawl4aiDockerClient:
        ...
    
    async def __aexit__(self, exc_type: Optional[type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -> None:
        ...
    


async def main(): # -> None:
    ...

if __name__ == "__main__":
    ...
